I"ˇ<h2 id="reqs">Reqs</h2>

<p>To execute this program you just need to install the following libraries:</p>
<ul>
  <li>Python3</li>
  <li>PyQt5</li>
  <li>numpy</li>
  <li>Pandas</li>
  <li>matplotlib</li>
</ul>

<h2 id="manual">Manual</h2>

<p>The main goal of this exercise is similar to the last proposed. To dig into the reinforcement learning basics.
In this case, the exercise is not that simple, since some times you will need to get farther from your goal to be able to reach it.
It makes thigs harder in terms of defining a reward function and the hyperparameters configuration to enable the agent to learn the either the optimal or a feasible sequence of steps to accomplish the objective.</p>

<p>In this case we are trying to solve the ‚ÄúmountainCar‚Äù problem proposed by openAIGym in which the objective is to teach a car how to reach the peak of a mountain just applying one of the following three actions:</p>
<ul>
  <li>apply a little force to right</li>
  <li>apply a little force to left</li>
  <li>do nothing</li>
</ul>

<p>For more details regarding this exercise, refer to the <a href="https://gym.openai.com/envs/MountainCar-v0/">openAIGym mountainCar website</a></p>

<p>Since it is still not possible to implement a deep reinforcement learning algorithm due to hardware constraints, the problem has been solved using a q learning algorithm.</p>

<p><strong>GRAPHS:</strong></p>

<p>To learn the behaviour of our agent based on what we are implementing it is important to have any kind of metric to measure the performance of each test.
To accomplish that we have three graphs in the upper part of a window that will be prompted each time the agent complete a configured number of maximum attempts to reach the goal.</p>

<ul>
  <li>
    <p>The first graph indicates how good the agent is learning a path so the total reward is for each run increases. This graph shows the total reward per run.</p>
  </li>
  <li>
    <p>The second graph indicates how many steps were needed in each run to either reach the goal or to fail. In our case, since we configured the environment to be reset when a total of 500 steps were executed, we wiil have an indication of failure when the run displayed reach a number of 500 steps. Otherwise it will indicate that the goal was reached before completing the 500 steps and that means a successfull simulation.</p>
  </li>
  <li>
    <p>The third graph indicates how close to the goal was the car in each run/simulation. It gives us an indication of how correlated is the reward and the performance of the simulations.</p>
  </li>
</ul>

<p>Additionally, ten matrices will be provided to indicate the qtables evolution, which will give us an idea of the learning evolution of our agent. Note that the matrix represents the following:</p>

<ul>
  <li>
    <p>In the horizontal axis, the car position is represented.</p>
  </li>
  <li>
    <p>In the vertical axis, the car speed is represented</p>
  </li>
  <li>
    <p>blue color means that the optimal action learned by the moment for that position-speed is ‚Äúdo nothing‚Äù</p>
  </li>
  <li>
    <p>green color means that the optimal action learned by the moment for that position-speed is ‚Äústep left‚Äù</p>
  </li>
  <li>
    <p>yellow color means that the optimal action learned by the moment for that position-speed is ‚Äústep right‚Äù</p>
  </li>
</ul>

<p><strong>CONFIGURATION</strong></p>

<p>Insted of not being really configurable, the code is easily modifiable to try different configurations. In order to do so, comments and constants with descriptive name are added at the beginning of the program.</p>

<p>However, to perform a try the suggestions are to modify the following:</p>

<ul>
  <li>
    <p>Hyperparameters</p>
  </li>
  <li>
    <p>Maximum number of steps per run</p>
  </li>
  <li>
    <p>Maximum number of runs before showing the results</p>
  </li>
  <li>
    <p>Reward function</p>
  </li>
</ul>

<p>In case of doubt and for seeking inspiration, you can check the ‚Äúresults document‚Äù uploaded in <a href="https://github.com/RoboticsLabURJC/2020-phd-ruben-lucas/tree/master/openAI_exercises/mountainCar/qlearning/results/">the github repository</a></p>

<h2 id="video">Video</h2>

<iframe width="560" height="420" src="http://www.youtube.com/embed/OifHupQe3KQ?color=white&amp;theme=light"></iframe>

<h2 id="code">Code</h2>

<p><a href="https://github.com/RoboticsLabURJC/2020-phd-ruben-lucas/tree/master/openAI_exercises/mountainCar/qlearning">mountainCar openAIGym exercise solved using qlearning</a></p>

<h2 id="results">Results</h2>

<p>As referenced in CONFIGURATION section, you can check the results of each qlearning configuration implemented <a href="https://github.com/RoboticsLabURJC/2020-phd-ruben-lucas/tree/master/openAI_exercises/mountainCar/qlearning/results/">here</a></p>

<p>As you will see, plenty of different hyperparameters configurations and reward functions were tried based on the interpretation of the ongoing results.
Finally, the best approximation was surprisingly found based on an error designing the reward function which consist of a gap between two levels of the reward function which make the agent learn faster that the optimal reward will be at the top of the mountain.</p>

<p>Additionally, we discover that the agent needs time and exploration to find a sequence of steps since the possibilities are really high and most of them drive to a failure simulation.</p>

<p><span style="color:green"><em>Feel free to share a better implementation or discrepancies with our conclussions!! We are humbly open to learn more from more experts!!</em></span></p>

:ET