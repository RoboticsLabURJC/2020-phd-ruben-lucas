I"[<h2 id="reqs">Reqs</h2>

<p>To execute this program you just need to install the following libraries:</p>
<ul>
  <li>Python3</li>
  <li>PyQt5</li>
  <li>numpy</li>
  <li>Pandas</li>
  <li>matplotlib</li>
</ul>

<h2 id="manual">Manual</h2>

<p>The main goal of this kind of exercises is to learn how to develop a simple reinforced learning algorithm to make an agent learn the optimal path to the goal as soon as possible.</p>

<p><strong>GRAPHS:</strong></p>

<p>To learn the behaviour of our agent based on what we are implementing it is important to have any kind of metric to measure the performance of each test.
To accomplish that we have two graphs in the upper part of a window that will be prompted each time the agent complete a configured number of maximum attempts to reach the goal.</p>

<ul>
  <li>
    <p>The first graph indicates how good the agent is learning a path so the total reward is for each run increases. This graph shows the total reward per run.</p>
  </li>
  <li>
    <p>The second graph indicates how many steps were needed in each run to either reach the goal or fail (getting out of the board or stepping a bomb)</p>
  </li>
</ul>

<p><strong>BOARD:</strong></p>

<p>Nevertheless, to ease the analysis of each try, it is always a good idea to represent the real scenario as clos to reality as possible. For that reason, a board is represented in the down side of the window with the number of times our agent steps each cell of the board before finishing the run. To better have an idea of the final learning of our agent we will display the most recurrents paths followed and the number of occurrences of those paths.</p>

<p><strong>CONFIGURATION</strong></p>

<p>Instead of not being really configurable, the code is easily modifiable to try different scenarios and to visualize more or less number of occurrences. In order to do so, comments and constants with descriptive name are added at the beginning of the program.</p>

<h2 id="videos">Videos</h2>

<iframe width="560" height="315" src="https://www.youtube.com/embed/5pHcHyNFSP4" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe>

<p><br /></p>

<iframe width="560" height="315" src="https://www.youtube.com/embed/HHlRMhiZWCM" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe>

<h2 id="code">Code</h2>

<p><a href="https://github.com/RoboticsLabURJC/2020-phd-ruben-lucas/tree/master/robot_mesh">Path learning to reach a goal using qlearning</a></p>

<h2 id="results">Results</h2>

<p>The easiest and fastest way to solve this problem was proved to be q learning. After some hyperparameters tunnings to improve the performance and some differents maps tested, the chosen map to exemplify the problem results is the following:</p>

<p><img src="/assets/images/results_images/robotmesh/map.png" alt="map" class="img-responsive" /></p>

<p>Being the green cells the start and end points, the black cells the bombs the “robot” must avoid and the blue cells the optimal path to achieve the goal.
The hyperparameters winners are the following:</p>

<ul>
  <li>GAMMA = 0.95</li>
  <li>LEARNING RATE = 0.9</li>
  <li>EXPLORATION_MIN = 0.000001</li>
  <li>EXPLORATION_DECAY = 0.999</li>
</ul>

<p>And the results, as shown in the video are indicated within the following graphics:</p>

<p><img src="/map.png" alt="results" class="img-responsive" /></p>

<p>As you can see, the algorithm converges around the simulation 450, from which all the paths followed were the optimal.</p>

<p>After this experiment I felt it was not enough and Sarsa was implemented to solve the same problem.
Note that Sarsa tries to converge with as less risk as possible, avoiding paths close to a low reward. For that reason, the map shown in the q learning example never converged using Sarsa. Once the map was a little more clear with wider paths to the goal, Sarsa got to learn a path to the goal even not being the optimal one.
You can find the explanation of this in the best answer to <a href="https://stats.stackexchange.com/questions/326788/when-to-choose-sarsa-vs-q-learning">this question</a>.</p>

<p><span style="color:green"><em>Feel free to share a better implementation or discrepancies with our conclussions!! We are humbly open to learn more from more experts!!</em></span></p>

:ET