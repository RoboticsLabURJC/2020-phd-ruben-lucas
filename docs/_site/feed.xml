<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.1.1">Jekyll</generator><link href="http://localhost:4000/2020-phd-ruben-lucas/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/2020-phd-ruben-lucas/" rel="alternate" type="text/html" /><updated>2022-01-15T16:29:46-08:00</updated><id>http://localhost:4000/2020-phd-ruben-lucas/feed.xml</id><title type="html">Robotics Lab URJC</title><subtitle>Programming Robot Intelligence</subtitle><author><name>Rubén Lucas</name></author><entry><title type="html">Projects polishing pre-integration with RL-Studio 1.1.0 (month 14)</title><link href="http://localhost:4000/2020-phd-ruben-lucas/your%20category/projects-polishing/" rel="alternate" type="text/html" title="Projects polishing pre-integration with RL-Studio 1.1.0 (month 14)" /><published>2022-01-14T00:00:00-08:00</published><updated>2022-01-14T00:00:00-08:00</updated><id>http://localhost:4000/2020-phd-ruben-lucas/your%20category/projects-polishing</id><content type="html" xml:base="http://localhost:4000/2020-phd-ruben-lucas/your%20category/projects-polishing/">&lt;p&gt;We are about to integrate the robot mesh and mountain car into RL-Studio 1.1.0, so no new problems will be addressed.
However, while revisitting the problems and algorithms already integrated into RL-Studio 0.1.0 (Note that the review was needed
because I came back from a job movement and I needed to pull all the repositories and reinstall the environment again) some
bugsfixes and improvements in “mountain_car” and minor code cleaning in “robot_mesh” were performed.&lt;/p&gt;

&lt;h2 id=&quot;lectures&quot;&gt;Lectures&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;First week of &lt;a href=&quot;https://www.coursera.org/learn/algorithms-part1/&quot;&gt;coursera princeton algorithms course&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;lab-work&quot;&gt;Lab work&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;The modifications and more relevant adjustments of the mountain_car problem have been documented in its &lt;a href=&quot;https://github.com/RoboticsLabURJC/2020-phd-ruben-lucas/tree/master/RL_Unibotics/RL-Studio/mountain_car/results.docx&quot;&gt;q_learning analysis report&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;</content><author><name>Tony Stark</name></author><category term="your category" /><category term="tag 1" /><category term="tag 2" /><category term="tag 3" /><category term="tag 4" /><summary type="html">Projects fixes, algorithms fine tuning and algorighms course</summary></entry><entry><title type="html">Final implementation of mountain_car basic scenario (Month 13 - Second half)</title><link href="http://localhost:4000/2020-phd-ruben-lucas/your%20category/mountain_car_final_migration_to_RLStudio_0.1.0/" rel="alternate" type="text/html" title="Final implementation of mountain_car basic scenario (Month 13 - Second half)" /><published>2021-10-25T00:00:00-07:00</published><updated>2021-10-25T00:00:00-07:00</updated><id>http://localhost:4000/2020-phd-ruben-lucas/your%20category/mountain_car_final_migration_to_RLStudio_0.1.0</id><content type="html" xml:base="http://localhost:4000/2020-phd-ruben-lucas/your%20category/mountain_car_final_migration_to_RLStudio_0.1.0/">&lt;p&gt;The goals of this two weeks were:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;get a productive solution for the mountain-car problem in RL-Studio 0.1.0.&lt;/li&gt;
  &lt;li&gt;Allig whith the rest of the JDeRobot team so we cooperate to evolve RL-Studio.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Regarding the mountain car adaptation/migration&lt;/strong&gt;, the following actions has been performed to make the simple environment built in the previous two weeks work:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Stable robot was imported an modified and the environment was refined to make the simulation controlable.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The statistics are propperly displayed while the simulation is run (in a different thread)&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;After lot of work and tunings to make it learn, the brain is finally learning!!&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;A new mode to execute actions manually was created (pressing 0 and enter the robot goes in one direction and pressiong 2 and enter it goes in the other direction). The name of the scrip is manual_runner.py&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Note that, when the new version of RL-Studio evolved by Pedro is updated, this and the robot mesh implementations will be
adapted and a new video to show this and the next two weeks work will be provided in “projects” section of this blog&lt;/p&gt;

&lt;h2 id=&quot;lectures&quot;&gt;Lectures&lt;/h2&gt;

&lt;p&gt;No new lectures were used this two (actually three) weeks&lt;/p&gt;

&lt;h2 id=&quot;lab-work&quot;&gt;Lab work&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;This week, the laboratory work consisted of keeping implementing the adapted/migrated to Rl-Studio mountain-car problem. &lt;strong&gt;Finally successfully!!!!&lt;/strong&gt;.&lt;/li&gt;
&lt;/ul&gt;</content><author><name>Rubén Lucas</name></author><category term="your category" /><category term="tag 1" /><category term="tag 2" /><category term="tag 3" /><category term="tag 4" /><summary type="html">Implemented first prototype for mountain car in RL-Studio</summary></entry><entry><title type="html">Still migrating mountain car to RL-Studio and year 1 phd formalization (Month 13 - First half)</title><link href="http://localhost:4000/2020-phd-ruben-lucas/your%20category/year_summary_and_mountain_car_debugging/" rel="alternate" type="text/html" title="Still migrating mountain car to RL-Studio and year 1 phd formalization (Month 13 - First half)" /><published>2021-10-04T00:00:00-07:00</published><updated>2021-10-04T00:00:00-07:00</updated><id>http://localhost:4000/2020-phd-ruben-lucas/your%20category/year_summary_and_mountain_car_debugging</id><content type="html" xml:base="http://localhost:4000/2020-phd-ruben-lucas/your%20category/year_summary_and_mountain_car_debugging/">&lt;p&gt;The goals of this month are:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;enroll in the doctoral program for the second year and pay the fees.&lt;/li&gt;
  &lt;li&gt;Propperly share the robot mesh implementation and videos both in github (via pull request) and slack (via forum posting)&lt;/li&gt;
  &lt;li&gt;write the first prototype of the investigation plan which will be followed during the following years to accomplish the final thesis.&lt;/li&gt;
  &lt;li&gt;progress with mountain car problem migration to RL-Studio 0.1.0.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;The enrollment and fees payment was accomplished.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;The robot mesh implementation in rl-studio was shared with the teammates both in github and slack&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Regarding the investigation plan&lt;/strong&gt;, it was written and can be found in the &lt;a href=&quot;https://gestion.urjc.es/RAPI/faces/task-flow-alumno/planesInvestigacion&quot;&gt;URJC platform&lt;/a&gt;. The overall idea is the following:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Year 1: Study of the state of the art for the problem of automatic robot behavior. In-depth study of current deep learning and reinforcement learning techniques to have a clear basis on which to develop the research. Creation of an objective measurement tool to evaluate the developed solutions.  First experiments with constrained circuits and solutions based on classical reinforcement learning for a mobile terrestrial robot.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;2nd Year: Implementation in a complex simulated environment of classical reinforcement learning and deep learning algorithms to solve basic practical problems that will allow to gather a number of techniques that will provide the PhD student with the necessary tools to tackle complex problems. It will also contribute to the development of the tool to provide it with the necessary power to facilitate the implementation of these complex problems that will be addressed in the coming years.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Year 3: Development of solutions combining deep learning and reinforcement learning to provide robustness to the robot for the previously developed environments and formalization of the solved problems in order to serve as a basis for solving future problems and to clearly illustrate the work done so that it can be used for teaching purposes.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;4th Year: Application of what has been previously learned to real environments, with real physical robots and resolution of more complex problems in simulators.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Year 5: Continuation of research of complex solutions transferred to real mobile robots after extraction of knowledge in simulation and application of the lessons learned to real environments that have a utility for the end user&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;That said, this plan is likely to be modified and extended in the future.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Regarding the mountain car adaptation/migration&lt;/strong&gt;, the following actions has been performed:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Create new environment simpler and feasibler trying to make the algorithm able to solve it before analysing what has to be done to solve the more compmlex one.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Include a statistics display after the 2 hours trainig to enable the algorithm monitoring and the subsequent analysis of what is and what is not working as expected.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Some minor fixes to the qlearning implementation (still insufficient)&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;lectures&quot;&gt;Lectures&lt;/h2&gt;

&lt;p&gt;The following lecture was useful to understand some concepts abut pickle files, used in RL-Studio.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://networkx.org/documentation/stable/reference/readwrite/gpickle.html&quot;&gt;pickle documentation&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Additionally, find below a forum were pull requests are explained and was useful to propperly share material with the teammates&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://opensource.com/article/19/7/create-pull-request-github&quot;&gt;how to create a pull request in github&lt;/a&gt;. Note that in RL-Studio team, this procedure also includes creating an incidence in the original repository and making the pull request from a branch named with this incidence id in the forked/cloned repository.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;lab-work&quot;&gt;Lab work&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;This week, the laboratory work consisted of keeping implementing the adapted/migrated to Rl-Studio mountain-car problem. &lt;strong&gt;No consistent results yet&lt;/strong&gt;.&lt;/li&gt;
&lt;/ul&gt;</content><author><name>Rubén Lucas</name></author><category term="your category" /><category term="tag 1" /><category term="tag 2" /><category term="tag 3" /><category term="tag 4" /><summary type="html">Keep working on mountain car (adapted to RL-Studio version) and formalize the phd burocracy</summary></entry><entry><title type="html">Migrating to new RLStudio and migrating mountain car to RLStudio 0.1.0 (Month 12)</title><link href="http://localhost:4000/2020-phd-ruben-lucas/your%20category/RLStudio_Migration_and_Mountain_car/" rel="alternate" type="text/html" title="Migrating to new RLStudio and migrating mountain car to RLStudio 0.1.0 (Month 12)" /><published>2021-09-14T00:00:00-07:00</published><updated>2021-09-14T00:00:00-07:00</updated><id>http://localhost:4000/2020-phd-ruben-lucas/your%20category/RLStudio_Migration_and_Mountain_car</id><content type="html" xml:base="http://localhost:4000/2020-phd-ruben-lucas/your%20category/RLStudio_Migration_and_Mountain_car/">&lt;p&gt;The goals of this month are:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Migration of the robot_mesh problem to the new RLStudio 0.1.0&lt;/li&gt;
  &lt;li&gt;getting the mountain car problem working in RLStudio 0.1.0&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;In the meanwhile:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Some ros tutorials has been done to understand how the communication with
the robots and the environment occurs.&lt;/li&gt;
  &lt;li&gt;some other modifications were suggested to be included in the Nacho
TFM:
    &lt;ol&gt;
      &lt;li&gt;Install manual miss some steps&lt;/li&gt;
      &lt;li&gt;There is one logs folder not created which gives error when executing the train_qlearn.py script&lt;/li&gt;
      &lt;li&gt;The lines 46 and 49 of train_qlearn requires a modification&lt;/li&gt;
    &lt;/ol&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;In spite of completing the mountain car migration to rl-studio and customized to better ilustrate the problem adding a local minimum, the agent is not learning as expected, so besides the mountain car code migration, it will need to be adaptated so it completes the problem (same problem concept but different “now in 3D” scenario)&lt;/p&gt;

&lt;h2 id=&quot;lectures&quot;&gt;Lectures&lt;/h2&gt;

&lt;p&gt;The only formal lecture done this month was for understanding the inference accomplished by Nacho in his TFM:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.pythonprogramming.net/q-learning-analysis-reinforcement-learning-python-tutorial&quot;&gt;qlearning inference with python tutorial&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;lab-work&quot;&gt;Lab work&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;http://wiki.ros.org/turtlesim/Tutorials&quot;&gt;basic ros tutorial&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://gazebosim.org/tutorials/?tut=ros_comm&quot;&gt;ros deeper practice in gazebo&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;</content><author><name>Rubén Lucas</name></author><category term="your category" /><category term="tag 1" /><category term="tag 2" /><category term="tag 3" /><category term="tag 4" /><summary type="html">Run Nacho TFM in new RLStudio and adapt mountain car made problem to work on it</summary></entry><entry><title type="html">Learning about RLStudio with basic robot mesh problem (months 10 and 11)</title><link href="http://localhost:4000/2020-phd-ruben-lucas/your%20category/rlstudio_robotmesh/" rel="alternate" type="text/html" title="Learning about RLStudio with basic robot mesh problem (months 10 and 11)" /><published>2021-07-26T00:00:00-07:00</published><updated>2021-07-26T00:00:00-07:00</updated><id>http://localhost:4000/2020-phd-ruben-lucas/your%20category/rlstudio_robotmesh</id><content type="html" xml:base="http://localhost:4000/2020-phd-ruben-lucas/your%20category/rlstudio_robotmesh/">&lt;p&gt;The goal of this month is to finally get a tangible work for robot_mesh problem.
In previous weeks, a robot was trained using qlearning to learn how to run from an origin to a destination
skiping some obstacles in the shortest path posible. Now, this problem has been migrated to not only get the statistics,
but also visualize the training and inference of this robot.
Additionally, the RL-Studio installation manual has been updated to skip some errors.
By the way, the computer bought last month is not yet ready… Dealing with swindlers.&lt;/p&gt;

&lt;h2 id=&quot;lectures&quot;&gt;Lectures&lt;/h2&gt;

&lt;p&gt;The lectures of this two months have been:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://gsyc.urjc.es/jmplaza/students/tfm-reinforcementlearning-conduccion_autonoma-ignacio_arranz-2020.pdf&quot;&gt;TFM Nacho Arranz&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://towardsdatascience.com/reinforcement-learning-generalisation-on-continuing-tasks-ffb9a89d57d0&quot;&gt;RL generalisation&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://en.wikipedia.org/wiki/Quaternion&quot;&gt;What is a Quaternion?&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;lab-work&quot;&gt;Lab work&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://github.com/JdeRobot/RL-Studio&quot;&gt;install rl-studio&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://quaternions.online/&quot;&gt;quaternion calculator&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://www.programmersought.com/article/36746345772/&quot;&gt;Use Gazebo to build a track and control the car to complete the race&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;</content><author><name>Rubén Lucas</name></author><category term="your category" /><category term="tag 1" /><category term="tag 2" /><category term="tag 3" /><category term="tag 4" /><summary type="html">Run Nacho TFM in RLStudio and adapt robot_mesh made problem to work on it</summary></entry><entry><title type="html">Introduction to gazebo and rl-studio (months 8 and 9)</title><link href="http://localhost:4000/2020-phd-ruben-lucas/your%20category/gazebo_and_rl-studio/" rel="alternate" type="text/html" title="Introduction to gazebo and rl-studio (months 8 and 9)" /><published>2021-05-25T00:00:00-07:00</published><updated>2021-05-25T00:00:00-07:00</updated><id>http://localhost:4000/2020-phd-ruben-lucas/your%20category/gazebo_and_rl-studio</id><content type="html" xml:base="http://localhost:4000/2020-phd-ruben-lucas/your%20category/gazebo_and_rl-studio/">&lt;p&gt;The goal of this month is to get in touch with gazebo and rl-studio in order to later work with this and develop our own artifial intelligence models making use of all the gazebo benefits.
Since these two months have been quite chaotic (vacations + computer fault) the progress to show is basically none. Hopefully it will ramp up soon because we will be working with a new PHD mate (Pedro) from now on. 
And last but not least. This week we got a new TOP computer with the following characteristics:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;RAM - 32GB DDR4 3200Mhz&lt;/li&gt;
  &lt;li&gt;HARD DISK DRIVE - HDD 2TB&lt;/li&gt;
  &lt;li&gt;SOLID STATE DRIVE - SSD m.2 NVME 500GB&lt;/li&gt;
  &lt;li&gt;CPU - AMD Ryzen7 5800X&lt;/li&gt;
  &lt;li&gt;GPU - RTX 6800 rx&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;lectures&quot;&gt;Lectures&lt;/h2&gt;

&lt;p&gt;The lectures of this two months have been:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://gsyc.urjc.es/jmplaza/students/tfm-reinforcementlearning-conduccion_autonoma-ignacio_arranz-2020.pdf&quot;&gt;TFM Nacho Arranz&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;http://gazebosim.org/tutorials&quot;&gt;some tutorials contained in gazebosim website&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;lab-work&quot;&gt;Lab work&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://github.com/JdeRobot/RL-Studio&quot;&gt;install rl-studio&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;http://gazebosim.org/tutorials?tut=build_robot&amp;amp;cat=build_robot&quot;&gt;create your own basic robot&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;</content><author><name>Rubén Lucas</name></author><category term="your category" /><category term="tag 1" /><category term="tag 2" /><category term="tag 3" /><category term="tag 4" /><summary type="html">Learning about gazebo and rl-studio to later develop in this environment</summary></entry><entry><title type="html">Study of first section of sutton book (month 7)</title><link href="http://localhost:4000/2020-phd-ruben-lucas/your%20category/rl_basics_study/" rel="alternate" type="text/html" title="Study of first section of sutton book (month 7)" /><published>2021-03-21T00:00:00-07:00</published><updated>2021-03-21T00:00:00-07:00</updated><id>http://localhost:4000/2020-phd-ruben-lucas/your%20category/rl_basics_study</id><content type="html" xml:base="http://localhost:4000/2020-phd-ruben-lucas/your%20category/rl_basics_study/">&lt;p&gt;The goal of this month is to fully read the first section of sutton book (8 chapters) and to fully understand
the basics of reincorcement learning. Once we got enough background, it will be easier to follow the good practices
and state-of-the-art of reinforcement learning. Additionally, since more algorithms will be known, more tools will
be at our hands so we can achieve better results for each one of the exercises and projects that will be accomplished.&lt;/p&gt;

&lt;p&gt;Additionally, to prove the know-how improvement, the mountain car exercise developed in previous posts will be revisited
and redesigned to achieve better results either modifying the current proposed algorithm or using a more suitable one.&lt;/p&gt;

&lt;h2 id=&quot;lectures&quot;&gt;Lectures&lt;/h2&gt;

&lt;p&gt;Since the work of this month consisted of learning from the sutton book referenced in &lt;a href=&quot;https://roboticslaburjc.github.io/2020-phd-ruben-lucas/about/&quot;&gt;resources section&lt;/a&gt;, what we are gathering in this section is a summary of headlines with the key lessons learned from this reading. (Note that there were some lessons already learned in previous iterations and note also that a summary of all the lessons learned in each chapter are provided in the book in the last item of every chapter).&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;&lt;span style=&quot;text-decoration: underline&quot;&gt;CHAPTER 1&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Subsection 1.5:&lt;/strong&gt; Introduction to Reinforcement learning with Tic-Tac-Toe example.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;&lt;span style=&quot;text-decoration: underline&quot;&gt;CHAPTER 2&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Subsection 2.1:&lt;/strong&gt; The importance of balancing between exploration and exploitation and some simple methods to do so.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Subsections 2.2, 2.3 and 2.5:&lt;/strong&gt; Introduction to stationary vs nonstationary problems.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Subsection 2.3:&lt;/strong&gt; The importance of e-greedy selection in a RL policy to increase the average reward taking benefit of exploration.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Subsection 2.6:&lt;/strong&gt; The importance of the assigned initial values to each scenario state.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Subsection 2.7:&lt;/strong&gt; UBC as an example of deterministically exploration policy.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;&lt;span style=&quot;text-decoration: underline&quot;&gt;CHAPTER 3&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Subsection 3.2:&lt;/strong&gt; It is not a good practice to give higher rewards to the agent when the performed action brings it closer to the goal instead of rewarding it for those actions which next state is actually the goal.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Subsections 3.5 and 3.6:&lt;/strong&gt; Introduction to the Bellman equation and value and quality concepts (already known but included in the summary due to its importance and the good explanation in the book).&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Subsection 3.7:&lt;/strong&gt; The Bellman equation is not feasible to be solved in plenty of real situations, so approximation may be needed&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;&lt;span style=&quot;text-decoration: underline&quot;&gt;CHAPTER 4&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Concepts of policy, value iteration and policy iteration.
For a better understanding of the difference between value iteration and policy iteration, this &lt;a href=&quot;https://stackoverflow.com/questions/37370015/what-is-the-difference-between-value-iteration-and-policy-iteration#:~:text=As%20much%20as%20I%20understand,the%20reward%20of%20that%20policy.&quot;&gt;forum discussion&lt;/a&gt; may be useful.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;&lt;span style=&quot;text-decoration: underline&quot;&gt;CHAPTER 5&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Subsections 5.1, 5.2 and 5.3:&lt;/strong&gt; Introduction to the Monte Carlo algorihm and the importance of exploring to discover the optimal action when having an (a-priori) unknown model.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Subsections 5.1, 5.2 and 5.3:&lt;/strong&gt; The importance of e-greedy and e-soft policies when using the Monte Carlo method.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Subsection 5.5:&lt;/strong&gt; Off-policy Vs On-policy methods and introduction to importance sampling (ordinary and weighted).&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;&lt;span style=&quot;text-decoration: underline&quot;&gt;CHAPTER 6&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Subsections 6.1, 6.2, 6.3 and 6.4:&lt;/strong&gt; TD(0) vs DP and Monte Carlo methods.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Subsections 6.4-&amp;gt;:&lt;/strong&gt;  Sarsa (and the following takeaway: if St+1 is terminal, then Q(St+1)=0), expected sarsa, Q-learning, and double learning&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Note that in this chapter, there are some specially interesting exercises to revisit (random walk, cliff mountain, driving home, windy windworld, etc.) all of them with a suggested algorithm to be applied.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;&lt;span style=&quot;text-decoration: underline&quot;&gt;CHAPTER 7&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Subsection 7.0:&lt;/strong&gt; Introduction to the concept of n-step bootstraping.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Subsections 7.2 and 7.3:&lt;/strong&gt;  n-step sarsa and n-step expected sarsa (derived from the prediction of TD-n step - intermediate
algorithm between monte-carlo and TD(0)).&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Subsection 7.5:&lt;/strong&gt; The n-step tree backup algorithm.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;&lt;span style=&quot;text-decoration: underline&quot;&gt;CHAPTER 8&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Subsection 8.1:&lt;/strong&gt; Consolidation of model-free Vs model-based and learning/planing paradigms + Introduction to distribution-model Vs sample-model.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Subsections 8.2 and 8.3:&lt;/strong&gt; Dyna-q and Dyna-q+: how to apply planning and direct RL learning simultaneously.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Subsection 8.4:&lt;/strong&gt; Prioritized sweeping.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Subsection 8.5:&lt;/strong&gt; Useful diagram to recap about some learned algorithms and its classification (Figure 8.6: Backup diagrams for all the one-step updates considered in this book).&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Subsections 8.6 and 8.7:&lt;/strong&gt; Introduction to trajectory sampling and RTDP as a given example.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Subsection 8.8:&lt;/strong&gt; Background planning vs decision-time planning.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;strong&gt;&lt;span style=&quot;text-decoration: underline&quot;&gt;BONUS&lt;/span&gt;&lt;/strong&gt;&lt;br /&gt;
While reading sutton book, a &lt;a href=&quot;https://shap.readthedocs.io/en/latest/&quot;&gt;really interesting tool&lt;/a&gt; for understanding how a machine learning algorithm actually learned and &lt;a href=&quot;https://www.researchgate.net/publication/349113191_Explainable_Reinforcement_Learning_for_Longitudinal_Control&quot;&gt;its application to reinforcement learning algorithms&lt;/a&gt; was found. Check it out!&lt;/p&gt;

&lt;h2 id=&quot;lab-work&quot;&gt;Lab work&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://roboticslaburjc.github.io/2020-phd-ruben-lucas/projects/2021-03-21-revisited_mountain_car&quot;&gt;Mountain car exercise revisited&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</content><author><name>Rubén Lucas</name></author><category term="your category" /><category term="tag 1" /><category term="tag 2" /><category term="tag 3" /><category term="tag 4" /><summary type="html">Reinforcement of RL basics to be able to perform good practices when developing</summary></entry><entry><title type="html">Projects standarization (month 6)</title><link href="http://localhost:4000/2020-phd-ruben-lucas/your%20category/standarization/" rel="alternate" type="text/html" title="Projects standarization (month 6)" /><published>2021-02-21T00:00:00-08:00</published><updated>2021-02-21T00:00:00-08:00</updated><id>http://localhost:4000/2020-phd-ruben-lucas/your%20category/standarization</id><content type="html" xml:base="http://localhost:4000/2020-phd-ruben-lucas/your%20category/standarization/">&lt;p&gt;The goal of this month is to have an homogeneus project structure and an easy to follow projects
coding plan (unwritten guidelines) so all of the exercises are organized in the same way.
Additionally, the introdction, QLearning and Sarsa sections of the sutton book have been reviewed besides the other sources included in “lectures” section.&lt;/p&gt;

&lt;h2 id=&quot;lectures&quot;&gt;Lectures&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://www.amazon.es/Reinforcement-Learning-Introduction-Richard-Sutton/dp/0262039249&quot;&gt;Reinforcement Learning: An Introduction&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;http://fumblog.um.ac.ir/gallery/839/weatherwax_sutton_solutions_manual.pdf&quot;&gt;Solutions to sutton book proposed exercises&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://stats.stackexchange.com/questions/189067/how-to-make-a-reward-function-in-reinforcement-learning&quot;&gt;How to build reward function&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://towardsdatascience.com/proximal-policy-optimization-tutorial-part-1-actor-critic-method-d53f9afffbf6&quot;&gt;proximal policy optimization with actor critic algorithm example&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://thegradient.pub/the-promise-of-hierarchical-reinforcement-learning/&quot;&gt;Hierarchichal reinforcement learning&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;lab-work&quot;&gt;Lab work&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://github.com/RoboticsLabURJC/2020-phd-ruben-lucas/tree/master/&quot;&gt;Project folders structure modified&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://github.com/RoboticsLabURJC/2020-phd-ruben-lucas/tree/master/RL_Unibotics/roboticsLab_exercises/robot_mesh&quot;&gt;robot_mesh project refactored&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;</content><author><name>Rubén Lucas</name></author><category term="your category" /><category term="tag 1" /><category term="tag 2" /><category term="tag 3" /><category term="tag 4" /><summary type="html">Standarizing the project folders and the exercises code structure</summary></entry><entry><title type="html">QLearning (month 5 · Weeks 3 and 4)</title><link href="http://localhost:4000/2020-phd-ruben-lucas/your%20category/mountainBall_qlearning/" rel="alternate" type="text/html" title="QLearning (month 5 · Weeks 3 and 4)" /><published>2021-02-07T00:00:00-08:00</published><updated>2021-02-07T00:00:00-08:00</updated><id>http://localhost:4000/2020-phd-ruben-lucas/your%20category/mountainBall_qlearning</id><content type="html" xml:base="http://localhost:4000/2020-phd-ruben-lucas/your%20category/mountainBall_qlearning/">&lt;p&gt;The goal of this two weeks is to create owr own environment with its own physics so
we can lately modify the mountain heigh, the car mass, the maximum velocity, the
number of mountains, the goal to be reached, etc.&lt;/p&gt;

&lt;h2 id=&quot;lectures&quot;&gt;Lectures&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;http://www.sc.ehu.es/sbweb/fisica/dinamica/con_mlineal/cuna/cuna.htm&quot;&gt;Physics basics in terms of movemetnt forces&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;lab-work&quot;&gt;Lab work&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/RoboticsLabURJC/2020-phd-ruben-lucas/tree/master/RL_Unibotics/roboticsLab_exercises/mountain_car&quot;&gt;mountainBall configurable environment and similar to mountainCar environment with qlearning solution&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</content><author><name>Rubén Lucas</name></author><category term="your category" /><category term="tag 1" /><category term="tag 2" /><category term="tag 3" /><category term="tag 4" /><summary type="html">Creating our own environment to freely configure our own problem</summary></entry><entry><title type="html">QLearning (month 5 · Weeks 1 and 2)</title><link href="http://localhost:4000/2020-phd-ruben-lucas/your%20category/mountainCar_qlearning/" rel="alternate" type="text/html" title="QLearning (month 5 · Weeks 1 and 2)" /><published>2021-01-22T00:00:00-08:00</published><updated>2021-01-22T00:00:00-08:00</updated><id>http://localhost:4000/2020-phd-ruben-lucas/your%20category/mountainCar_qlearning</id><content type="html" xml:base="http://localhost:4000/2020-phd-ruben-lucas/your%20category/mountainCar_qlearning/">&lt;p&gt;The goal of this two weeks is to solve the mountainCar problem using a reinforced learning algorithm with no neural network. 
Besides that, this blog will be refined to better show the work progress and learnings.&lt;/p&gt;

&lt;h2 id=&quot;lectures&quot;&gt;Lectures&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://dibyaghosh.com/blog/probability/kldivergence.html&quot;&gt;KL Divergence for Machine Learning&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://datascience.stackexchange.com/questions/20296/cross-entropy-loss-explanation&quot;&gt;Cross entropy loss explanation&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://insights.daffodilsw.com/blog/machine-unlearning-what-it-is-all-about&quot;&gt;Unlearning phenomenom in machine learning&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;lab-work&quot;&gt;Lab work&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/RoboticsLabURJC/2020-phd-ruben-lucas/tree/master/RL_Unibotics/openAI_exercises/mountainCar/qlearning&quot;&gt;mountainCar openAIGym exercise with qlearning&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</content><author><name>Rubén Lucas</name></author><category term="your category" /><category term="tag 1" /><category term="tag 2" /><category term="tag 3" /><category term="tag 4" /><summary type="html">Solving a more complex problem using qlearning</summary></entry></feed>