<!doctype html>
<!--
  Minimal Mistakes Jekyll Theme 4.16.5 by Michael Rose
  Copyright 2013-2020 Michael Rose - mademistakes.com | @mmistakes
  Free for personal and commercial use under the MIT license
  https://github.com/mmistakes/minimal-mistakes/blob/master/LICENSE
-->
<html lang="en" class="no-js">
  <head>
    <meta charset="utf-8">

<!-- begin _includes/seo.html --><title>MountainCar openAI Gym exercise solved using q learning - Robotics Lab URJC</title>
<meta name="description" content="Note that q learning is not the best option to solve this exercise">



<meta property="og:type" content="article">
<meta property="og:locale" content="en_US">
<meta property="og:site_name" content="Robotics Lab URJC">
<meta property="og:title" content="MountainCar openAI Gym exercise solved using q learning">
<meta property="og:url" content="http://localhost:4000/2020-phd-ruben-lucas/projects/2021-01-22-mountainCar_qlearning/">


  <meta property="og:description" content="Note that q learning is not the best option to solve this exercise">







  <meta property="article:published_time" content="2021-01-22T00:00:00-08:00">





  

  


<link rel="canonical" href="http://localhost:4000/2020-phd-ruben-lucas/projects/2021-01-22-mountainCar_qlearning/">







  <script type="application/ld+json">
    {
      "@context": "https://schema.org",
      "@type": "Person",
      "name": "JdeRobot",
      "url": "http://localhost:4000/2020-phd-ruben-lucas",
      "sameAs": null
    }
  </script>







<!-- end _includes/seo.html -->


<link href="/2020-phd-ruben-lucas/feed.xml" type="application/atom+xml" rel="alternate" title="Robotics Lab URJC Feed">

<!-- https://t.co/dKP3o1e -->
<meta name="viewport" content="width=device-width, initial-scale=1.0">

<script>
  document.documentElement.className = document.documentElement.className.replace(/\bno-js\b/g, '') + ' js ';
</script>

<!-- For all browsers -->
<link rel="stylesheet" href="/2020-phd-ruben-lucas/assets/css/main.css">

<!--[if IE ]>
  <style>
    /* old IE unsupported flexbox fixes */
    .greedy-nav .site-title {
      padding-right: 3em;
    }
    .greedy-nav button {
      position: absolute;
      top: 0;
      right: 0;
      height: 100%;
    }
  </style>
<![endif]-->



    <!-- start custom head snippets -->

<!-- insert favicons. use https://realfavicongenerator.net/ -->
<link rel="icon" type="image/png" href="/assets/images/logo.png" sizes="16x16">
<!-- end custom head snippets -->

  </head>

  <body class="layout--single">
    <nav class="skip-links">
  <h2 class="screen-reader-text">Skip links</h2>
  <ul>
    <li><a href="#site-nav" class="screen-reader-shortcut">Skip to primary navigation</a></li>
    <li><a href="#main" class="screen-reader-shortcut">Skip to content</a></li>
    <li><a href="#footer" class="screen-reader-shortcut">Skip to footer</a></li>
  </ul>
</nav>

    <!--[if lt IE 9]>
<div class="notice--danger align-center" style="margin: 0;">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience.</div>
<![endif]-->

    

<div class="masthead">
  <div class="masthead__inner-wrap">
    <div class="masthead__menu">
      <nav id="site-nav" class="greedy-nav">
        
          <a class="site-logo" href="/2020-phd-ruben-lucas/"><img src="/2020-phd-ruben-lucas/assets/images/peloto.png" alt=""></a>
        
        <a class="site-title" href="/2020-phd-ruben-lucas/">
          Robotics Lab URJC
          <span class="site-subtitle">Programming Robot Intelligence</span>
        </a>
        <ul class="visible-links"><li class="masthead__menu-item">
              <a href="/2020-phd-ruben-lucas/logbook/" >LogBook</a>
            </li><li class="masthead__menu-item">
              <a href="/2020-phd-ruben-lucas/install/" >Projects</a>
            </li><li class="masthead__menu-item">
              <a href="/2020-phd-ruben-lucas/about/" >Resources</a>
            </li></ul>
        
        <button class="greedy-nav__toggle hidden" type="button">
          <span class="visually-hidden">Toggle menu</span>
          <div class="navicon"></div>
        </button>
        <ul class="hidden-links hidden"></ul>
      </nav>
    </div>
  </div>
</div>


    <div class="initial-content">
      



<div id="main" role="main">
  
  <div class="sidebar sticky">
  


<div itemscope itemtype="https://schema.org/Person">

  

  <div class="author__content">
    
      <h3 class="author__name" itemprop="name"></h3>
    
    
  </div>

  <div class="author__urls-wrapper">
    <button class="btn btn--inverse">Follow</button>
    <ul class="author__urls social-icons">
      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      <!--
  <li>
    <a href="http://link-to-whatever-social-network.com/user/" itemprop="sameAs" rel="nofollow noopener noreferrer">
      <i class="fas fa-fw" aria-hidden="true"></i> Custom Social Profile Link
    </a>
  </li>
-->
    </ul>
  </div>
</div>

  
    
      
      
      
      
    
    
      

<nav class="nav__list">
  
  <input id="ac-toc" name="accordion-toc" type="checkbox" />
  <label for="ac-toc">Toggle Menu</label>
  <ul class="nav__items">
    
      <li>
        
          <span class="nav__sub-title">LogBook</span>
        

        
        <ul>
          
            
            

            
            

            <li><a href="/2020-phd-ruben-lucas/logbook/" class="">Check it out</a></li>
          
        </ul>
        
      </li>
    
      <li>
        
          <span class="nav__sub-title">Projects</span>
        

        
        <ul>
          
            
            

            
            

            <li><a href="/2020-phd-ruben-lucas/install/" class="">AI Exercises</a></li>
          
        </ul>
        
      </li>
    
      <li>
        
          <span class="nav__sub-title">Resources</span>
        

        
        <ul>
          
            
            

            
            

            <li><a href="/2020-phd-ruben-lucas/references/" class="">Usefull resources</a></li>
          
        </ul>
        
      </li>
    
  </ul>
</nav>
    
  
  </div>


  <article class="page" itemscope itemtype="https://schema.org/CreativeWork">
    <meta itemprop="headline" content="MountainCar openAI Gym exercise solved using q learning">
    <meta itemprop="description" content="Note that q learning is not the best option to solve this exercise">
    <meta itemprop="datePublished" content="January 22, 2021">
    

    <div class="page__inner-wrap">
      
        <header>
          <h1 id="page-title" class="page__title" itemprop="headline">MountainCar openAI Gym exercise solved using q learning
</h1>
          
            <p class="page__meta"><i class="far fa-clock" aria-hidden="true"></i> 




  5 minute read
</p>
          
        </header>
      

      <section class="page__content" itemprop="text">
        
          <aside class="sidebar__right ">
            <nav class="toc">
              <header><h4 class="nav__title"><i class="fas fa-cog"></i> TOC installation</h4></header>
              <ul class="toc__menu">
  <li><a href="#reqs">Reqs</a></li>
  <li><a href="#manual">Manual</a></li>
  <li><a href="#video">Video</a></li>
  <li><a href="#code">Code</a></li>
  <li><a href="#results">Results</a></li>
</ul>
            </nav>
          </aside>
        
        <h2 id="reqs">Reqs</h2>

<p>To execute this program you just need to install the following libraries:</p>
<ul>
  <li>Python3</li>
  <li>PyQt5</li>
  <li>numpy</li>
  <li>Pandas</li>
  <li>matplotlib</li>
</ul>

<h2 id="manual">Manual</h2>

<p>The main goal of this exercise is similar to the last proposed. To dig into the reinforcement learning basics.
In this case, the exercise is not that simple, since some times you will need to get farther from your goal to be able to reach it.
It makes thigs harder in terms of defining a reward function and the hyperparameters configuration to enable the agent to learn the either the optimal or a feasible sequence of steps to accomplish the objective.</p>

<p>In this case we are trying to solve the “mountainCar” problem proposed by openAIGym in which the objective is to teach a car how to reach the peak of a mountain just applying one of the following three actions:</p>
<ul>
  <li>apply a little force to right</li>
  <li>apply a little force to left</li>
  <li>do nothing</li>
</ul>

<p>For more details regarding this exercise, refer to the <a href="https://gym.openai.com/envs/MountainCar-v0/">openAIGym mountainCar website</a></p>

<p>Since it is still not possible to implement a deep reinforcement learning algorithm due to hardware constraints, the problem has been solved using a q learning algorithm.</p>

<p><strong>GRAPHS:</strong></p>

<p>To learn the behaviour of our agent based on what we are implementing it is important to have any kind of metric to measure the performance of each test.
To accomplish that we have three graphs in the upper part of a window that will be prompted each time the agent complete a configured number of maximum attempts to reach the goal.</p>

<ul>
  <li>
    <p>The first graph indicates how good the agent is learning a path so the total reward is for each run increases. This graph shows the total reward per run.</p>
  </li>
  <li>
    <p>The second graph indicates how many steps were needed in each run to either reach the goal or to fail. In our case, since we configured the environment to be reset when a total of 500 steps were executed, we wiil have an indication of failure when the run displayed reach a number of 500 steps. Otherwise it will indicate that the goal was reached before completing the 500 steps and that means a successfull simulation.</p>
  </li>
  <li>
    <p>The third graph indicates how close to the goal was the car in each run/simulation. It gives us an indication of how correlated is the reward and the performance of the simulations.</p>
  </li>
</ul>

<p>Additionally, ten matrices will be provided to indicate the qtables evolution, which will give us an idea of the learning evolution of our agent. Note that the matrix represents the following:</p>

<ul>
  <li>
    <p>In the horizontal axis, the car position is represented.</p>
  </li>
  <li>
    <p>In the vertical axis, the car speed is represented</p>
  </li>
  <li>
    <p>blue color means that the optimal action learned by the moment for that position-speed is “do nothing”</p>
  </li>
  <li>
    <p>green color means that the optimal action learned by the moment for that position-speed is “step left”</p>
  </li>
  <li>
    <p>yellow color means that the optimal action learned by the moment for that position-speed is “step right”</p>
  </li>
</ul>

<p><strong>CONFIGURATION</strong></p>

<p>Insted of not being really configurable, the code is easily modifiable to try different configurations. In order to do so, comments and constants with descriptive name are added at the beginning of the program.</p>

<p>However, to perform a try the suggestions are to modify the following:</p>

<ul>
  <li>
    <p>Hyperparameters</p>
  </li>
  <li>
    <p>Maximum number of steps per run</p>
  </li>
  <li>
    <p>Maximum number of runs before showing the results</p>
  </li>
  <li>
    <p>Reward function</p>
  </li>
</ul>

<p>In case of doubt and for seeking inspiration, you can check the “results document” uploaded in <a href="https://github.com/RoboticsLabURJC/2020-phd-ruben-lucas/tree/master/RL_Unibotics/openAI_exercises/mountainCar/qlearning/results">the github repository</a></p>

<h2 id="video">Video</h2>

<iframe width="560" height="315" src="https://www.youtube.com/embed/OifHupQe3KQ" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe>

<h2 id="code">Code</h2>

<p><a href="https://github.com/RoboticsLabURJC/2020-phd-ruben-lucas/tree/master/RL_Unibotics/openAI_exercises/mountainCar/qlearning/">mountainCar openAIGym exercise solved using qlearning</a></p>

<h2 id="results">Results</h2>

<p>As referenced in CONFIGURATION section, you can check the results of each qlearning configuration implemented <a href="https://github.com/RoboticsLabURJC/2020-phd-ruben-lucas/tree/master/RL_Unibotics/openAI_exercises/mountainCar/qlearning/results">here</a></p>

<p>As you will see, plenty of different hyperparameters configurations and reward functions were tried based on the interpretation of the ongoing results.
Finally, the best approximation was surprisingly found based on an error designing the reward function which consist of a gap between two levels of the reward function which make the agent learn faster that the optimal reward will be at the top of the mountain.</p>

<p>Additionally, we discover that the agent needs time and exploration to find a sequence of steps since the possibilities are really high and most of them drive to a failure simulation.</p>

<p>To sum up, after trying practically everything, the following configuration has achieved the best results:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>MAX_RUNS=3000
MAXIMUM_STEPS=500

GAMMA = 0.95
LEARNING_RATE = 0.2

EXPLORATION_MAX = 1.0
EXPLORATION_MIN = 0.05
EXPLORATION_DECAY = 0.9995

def get_reward(state, step):
    if state[0] &gt;= 0.5:
        print("Car has reached the goal")
        return 500
    elif state[0]&lt;-0.7:
        return ((state[0]+0.7))
    elif state[0]&gt;-0.7 and state[0]&lt;-0.2:
        return 9*(state[0]+0.3)
    elif state[0]&gt;=-0.2:
        return (9*(state[0]+0.3))**2
</code></pre></div></div>

<p><img src="/2020-phd-ruben-lucas/assets/images/results_images/mountainCar/results.png" alt="results" class="img-responsive" /></p>

<p>And so, our conclussions are the following:</p>

<ul>
  <li>It is important to propperly learn the actions that are going to be executed more often, so you should control your exploration simulations to make sure it is happening</li>
  <li>In this case it is important to give high importance to the future rewards so it is propagated to previous less rewarded states importants also to reach the goal</li>
  <li>It is important to give time to the agent to learn the optimal path (in this case) increasing the maximum number of steps per simulation</li>
  <li>In cases where the randomly selection of actions is unlikely to guide your agent to the goal, it is important to give it clues so it approach more and more the goal. In this case we just got reasonably good results building a reward functions with several levels so the agent learn that:
    <ul>
      <li>Valley positions is something to avoid (no reward)</li>
      <li>Climbing to the left is not bad but it is not that good as climbing to the right (closer to the goal)</li>
      <li>Goal is the best state possible</li>
    </ul>
  </li>
  <li>And last (and probably least important) it is a good idea to keep a minimum exploration rate so you encourage your agent to not to give things as given even when it is in a later stage. In this way, in spite of doing it at a much lower speed, the agent will always keep learning.</li>
</ul>

<p><span style="color:green"><em>Feel free to share a better implementation or discrepancies with our conclussions!! We are humbly open to learn more from more experts!!</em></span></p>

        
      </section>

      <footer class="page__meta">
        
        
  


  
  
  

  <p class="page__taxonomy">
    <strong><i class="fas fa-fw fa-tags" aria-hidden="true"></i> Tags: </strong>
    <span itemprop="keywords">
    
      
      
      <a href="/2020-phd-ruben-lucas/tags/#tag-1" class="page__taxonomy-item" rel="tag">tag 1</a><span class="sep">, </span>
    
      
      
      <a href="/2020-phd-ruben-lucas/tags/#tag-2" class="page__taxonomy-item" rel="tag">tag 2</a><span class="sep">, </span>
    
      
      
      <a href="/2020-phd-ruben-lucas/tags/#tag-3" class="page__taxonomy-item" rel="tag">tag 3</a><span class="sep">, </span>
    
      
      
      <a href="/2020-phd-ruben-lucas/tags/#tag-4" class="page__taxonomy-item" rel="tag">tag 4</a>
    
    </span>
  </p>




  


  
  
  

  <p class="page__taxonomy">
    <strong><i class="fas fa-fw fa-folder-open" aria-hidden="true"></i> Categories: </strong>
    <span itemprop="keywords">
    
      
      
      <a href="/2020-phd-ruben-lucas/categories/#your-category" class="page__taxonomy-item" rel="tag">your category</a>
    
    </span>
  </p>


        
          <p class="page__date"><strong><i class="fas fa-fw fa-calendar-alt" aria-hidden="true"></i> Updated:</strong> <time datetime="2021-01-22T00:00:00-08:00">January 22, 2021</time></p>
        
      </footer>

      <section class="page__share">
  

  <a href="https://twitter.com/intent/tweet?text=MountainCar+openAI+Gym+exercise+solved+using+q+learning%20http%3A%2F%2Flocalhost%3A4000%2F2020-phd-ruben-lucas%2Fprojects%2F2021-01-22-mountainCar_qlearning%2F" class="btn btn--twitter" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" title="Share on Twitter"><i class="fab fa-fw fa-twitter" aria-hidden="true"></i><span> Twitter</span></a>

  <!--<a href="https://www.facebook.com/sharer/sharer.php?u=http%3A%2F%2Flocalhost%3A4000%2F2020-phd-ruben-lucas%2Fprojects%2F2021-01-22-mountainCar_qlearning%2F" class="btn btn--facebook" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" title="Share on Facebook"><i class="fab fa-fw fa-facebook" aria-hidden="true"></i><span> Facebook</span></a>-->

  <a href="https://www.linkedin.com/shareArticle?mini=true&url=http%3A%2F%2Flocalhost%3A4000%2F2020-phd-ruben-lucas%2Fprojects%2F2021-01-22-mountainCar_qlearning%2F" class="btn btn--linkedin" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" title="Share on LinkedIn"><i class="fab fa-fw fa-linkedin" aria-hidden="true"></i><span> LinkedIn</span></a>
</section>


      
  <nav class="pagination">
    
      <a href="/2020-phd-ruben-lucas/projects/2021-01-08-model_free_qlearning_algorithm/" class="pagination--pager" title="Robot following path to goal with q learning and sarsa
">Previous</a>
    
    
      <a href="/2020-phd-ruben-lucas/projects/2021-02-07-customized-mountainCar/" class="pagination--pager" title="MountainCar customized environment making use of OpenAI Gym libraries
">Next</a>
    
  </nav>

    </div>

    
  </article>

  
  
    <div class="page__related">
      <h4 class="page__related-title">You May Also Enjoy</h4>
      <div class="grid__wrapper">
        
          



<div class="grid__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title" itemprop="headline">
      
        <a href="/2020-phd-ruben-lucas/your%20category/mountainBall_qlearning/" rel="permalink">QLearning (month 5 · Weeks 3 and 4)
</a>
      
    </h2>
    
      <p class="page__meta"><i class="far fa-clock" aria-hidden="true"></i> 




  less than 1 minute read
</p>
    
    <p class="archive__item-excerpt" itemprop="description">creating our own environment to freely configure our own problem
</p>
  </article>
</div>

        
          



<div class="grid__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title" itemprop="headline">
      
        <a href="/2020-phd-ruben-lucas/your%20category/mountainCar_qlearning/" rel="permalink">QLearning (month 5 · Weeks 1 and 2)
</a>
      
    </h2>
    
      <p class="page__meta"><i class="far fa-clock" aria-hidden="true"></i> 




  less than 1 minute read
</p>
    
    <p class="archive__item-excerpt" itemprop="description">Solving a more complex problem using qlearning
</p>
  </article>
</div>

        
          



<div class="grid__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title" itemprop="headline">
      
        <a href="/2020-phd-ruben-lucas/your%20category/model_free_qlearning_algorithm/" rel="permalink">QLearning and Sarsa (month 4)
</a>
      
    </h2>
    
      <p class="page__meta"><i class="far fa-clock" aria-hidden="true"></i> 




  less than 1 minute read
</p>
    
    <p class="archive__item-excerpt" itemprop="description">Start playing with reinforcement learning
</p>
  </article>
</div>

        
          



<div class="grid__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title" itemprop="headline">
      
        <a href="/2020-phd-ruben-lucas/your%20category/current-work/" rel="permalink">Digging in (month 3)
</a>
      
    </h2>
    
      <p class="page__meta"><i class="far fa-clock" aria-hidden="true"></i> 




  less than 1 minute read
</p>
    
    <p class="archive__item-excerpt" itemprop="description">Start playing with reinforcement learning
</p>
  </article>
</div>

        
      </div>
    </div>
  
  
</div>

    </div>

    

    <div id="footer" class="page__footer">
      <footer>
        <!-- start custom footer snippets -->

<!-- end custom footer snippets -->
        <div class="page__footer-follow">
  <ul class="social-icons">
    

    
      
        
          <li><a href="https://www.linkedin.com/in/ruben-lucas-zaragoza/" rel="nofollow noopener noreferrer"><i class="fab fa-fw fa-linkedin-in" aria-hidden="true"></i> Linkedn</a></li>
        
      
        
          <li><a href="https://github.com/RoboticsLabURJC/2020-phd-ruben-lucas" rel="nofollow noopener noreferrer"><i class="fab fa-fw fa-github" aria-hidden="true"></i> GitHub</a></li>
        
      
        
          <li><a href="https://www.youtube.com/channel/UC8Kl0ECm4gjhxSozBqghQLQ" rel="nofollow noopener noreferrer"><i class="fab fa-fw fa-youtube" aria-hidden="true"></i> Youtube</a></li>
        
      
    

    <li><a href="/2020-phd-ruben-lucas/feed.xml"><!--<i class="fas fa-fw fa-rss-square" aria-hidden="true"></i> Feed--></a></li>
  </ul>
</div>

<div class="page__footer-copyright">&copy; 2021 JdeRobot. Powered by <a href="https://jekyllrb.com" rel="nofollow">Jekyll</a>, <a href="https://mademistakes.com/work/minimal-mistakes-jekyll-theme/" rel="nofollow">Minimal Mistakes</a> &amp; modified by <a href="https://github.com/igarag">NachoAz</a>.</div>

      </footer>
    </div>

    
  <script src="/2020-phd-ruben-lucas/assets/js/main.min.js"></script>
  <script src="https://kit.fontawesome.com/4eee35f757.js"></script>
  <script type="text/javascript" async
    src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-MML-AM_CHTML">
  </script>
  <script type="text/x-mathjax-config">
     MathJax.Hub.Config({
       extensions: ["tex2jax.js"],
       jax: ["input/TeX", "output/HTML-CSS"],
       tex2jax: {
         inlineMath: [ ['$','$'], ["\\(","\\)"] ],
         displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
         processEscapes: true
       },
       "HTML-CSS": { availableFonts: ["TeX"] }
     });
  </script>










  </body>
</html>
