experiments:

#  - config: config/config_training_followlane_bs_ppo_f1_carla.yaml
#    carla:
#      carla_client: 5013
#    carla_environments:
#      follow_lane:
#        town: [Town03, Town01, Town04]
#    retraining:
#      ppo:
#        retrain_ppo_tf_model_name: /home/ruben/Desktop/2020-phd-ruben-lucas/src/RL-Studio/rl_studio/checkpoints/follow_lane_carla_ppo_continuous_auto_carla_baselines/20250819-230238/model_830000_steps.zip
#    settings:
#      mode: retraining
#      algorithm: ppo_continuous
#      normalize: true
#      appended_states: 10
##      steps_to_decrease: 3000
##      decrease_min: -5
#      initial_std_w: -1
#      initial_std_v: -0.2
#      reward_params:
#        punish_braking: 2 # used as d_reward_power
#        punish_zig_zag_value: 2
#        punish_deviation: 5
#        beta_1: 0

#  - config: config/config_training_followlane_bs_ddpg_f1_carla.yaml
#    carla:
#      carla_client: 5013
#    carla_environments:
#      follow_lane:
#        town: [ Town03, Town01 ]
#    settings:
#      algorithm: ddpg
#      normalize: true
#      steps_to_decrease: 500
#      appended_states: 10
#      reward_params:
#        punish_braking: 1 # used as d_reward_power
#        punish_zig_zag_value: 5
#        punish_deviation: 10
#        beta_1: 0

  - config: config/config_training_followlane_bs_ppo_f1_carla.yaml
    carla:
      carla_client: 5013
    carla_environments:
      follow_lane:
        town: [Town03, Town01, Town04]
    settings:
      mode: training
      algorithm: ppo_continuous
      normalize: true
      appended_states: 10
      steps_to_decrease: 20000
      decrease_min: -4
      initial_std_w: -0.4
      initial_std_v: -0.2
      reward_params:
        punish_braking: 2 # used as d_reward_power
        punish_zig_zag_value: 0.5
        punish_deviation: 5
        beta_1: 0
    algorithm:
      ppo:
        gamma: 0.99
        epsilon: 0.2
        episodes_update: 50 # dooing nothing here
        replay_memory_size: 50_000
        memory_fraction: 0.20
        critic_lr: 0.0003 # This is the only one applying to both now
        actor_lr: 0.0002

  - config: config/config_training_followlane_bs_ppo_f1_carla.yaml
    carla:
      carla_client: 5013
    carla_environments:
      follow_lane:
        town: [Town03, Town01, Town04]
    settings:
      mode: training
      algorithm: ppo_continuous
      normalize: true
      appended_states: 10
      steps_to_decrease: 10000
      initial_std_w: -0.5
      initial_std_v: -0.5
      reward_params:
        punish_braking: 2 # used as d_reward_power
        punish_zig_zag_value: 2
        punish_deviation: 5
        beta_1: 0
    algorithm:
      ppo:
        gamma: 0.99
        epsilon: 0.3
        episodes_update: 50 # dooing nothing here
        replay_memory_size: 50_000
        memory_fraction: 0.20
        critic_lr: 0.00003 # This is the only one applying to both now
        actor_lr: 0.0002

#  - config: config/config_training_followlane_bs_sac_f1_carla.yaml
#    carla:
#      carla_client: 5013
#    carla_environments:
#      follow_lane:
#        town: [Town03, Town01, Town04]
#    settings:
#      algorithm: sac
#      normalize: true
#      initial_std_w: 0.5
#      initial_std_v: 0.8
#      steps_to_decrease: 4000
#      appended_states: 10
#      reward_params:
#        punish_braking: 2 # used as d_reward_power
#        punish_zig_zag_value: 0.5
#        punish_deviation: 5
#        beta_1: 0