experiments:

#  - config: config/config_training_followlane_bs_td3_f1_carla.yaml
#    carla:
#      carla_client: 4013
#    carla_environments:
#      follow_lane:
#        town: [Town01, Town03, Town04]
#    settings:
##      mode: retraining
#      algorithm: td3
#      normalize: true
#      net_arch: [128, 128, 128, 128, 128]
#      initial_std_w: 0.8
#      initial_std_v: 0.8
#      steps_to_decrease: 2000
#      appended_states: 10
#      reward_params:
#        punish_braking: 2 # used as d_reward_power
#        punish_zig_zag_value: 0.5
#        punish_deviation: 10
#        beta_1: 0
#
#  - config: config/config_training_followlane_bs_td3_f1_carla.yaml
#    carla:
#      carla_client: 4013
#    carla_environments:
#      follow_lane:
#        town: [Town01, Town03, Town04]
#    settings:
##      mode: retraining
#      algorithm: td3
#      normalize: true
#      net_arch: [128, 128, 128, 128, 128]
#      initial_std_w: 0.8
#      initial_std_v: 0.8
#      steps_to_decrease: 2000
#      appended_states: 10
#      reward_params:
#        punish_braking: 2 # used as d_reward_power
#        punish_zig_zag_value: 0.5
#        punish_deviation: 10
#        beta_1: 0

  - config: config/config_training_followlane_bs_td3_f1_carla.yaml
    carla:
      carla_client: 4013
    carla_environments:
      follow_lane:
        town: [Town01, Town03, Town04]
    settings:
#      mode: retraining
      algorithm: td3
      normalize: true
      net_arch: [128, 128, 128, 128, 128]
      initial_std_w: 0.8
      initial_std_v: 0.8
      steps_to_decrease: 4000
      seed: 888
      appended_states: 10
      reward_params:
        gamma: 0.9
        punish_braking: 2 # used as d_reward_power
        critic_lr: 0.0001
        punish_zig_zag_value: 0.5
        punish_deviation: 10
        beta_1: 0

  - config: config/config_training_followlane_bs_td3_f1_carla.yaml
    carla:
      carla_client: 4013
    carla_environments:
      follow_lane:
        town: [Town01, Town03, Town04]
    settings:
#      mode: retraining
      algorithm: td3
      normalize: true
      net_arch: [128, 128, 128, 128, 128]
      initial_std_w: 0.8
      initial_std_v: 0.8
      steps_to_decrease: 2000
      seed: 888
      appended_states: 10
      reward_params:
        gamma: 0.9
        critic_lr: 0.0003
        punish_braking: 2 # used as d_reward_power
        punish_zig_zag_value: 0.5
        punish_deviation: 10
        beta_1: 0